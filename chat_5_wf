#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Workflow RAG (LlamaIndex) con memoria persistente + reranker (cross-encoder),
sobre un CSV estilo Rotten Tomatoes.

- Storage: storage_5/  (índice + chat_store + manifest/fingerprints)
- LLM: OpenAI gpt-4o-mini-2024-07-18  (OPENAI_API_KEY requerido)
- Embeddings: sentence-transformers/all-MiniLM-L6-v2  (HuggingFace)
- Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2
- Memoria: ChatMemoryBuffer + SimpleChatStore (JSON en storage_5/chat_store.json)

Requisitos (ejemplo):
  pip install "llama-index>=0.11" llama-index-llms-openai llama-index-embeddings-huggingface
  pip install sentence-transformers python-dotenv pandas
"""

import os
import re
import json
import hashlib
import asyncio
import logging
from datetime import datetime
from typing import List, Optional, Literal

import pandas as pd
from dotenv import load_dotenv

# ---- LlamaIndex core & workflow ----
from llama_index.core import (
    Document,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.core.workflow import Workflow, step, Context, Event, StartEvent, StopEvent

# LLM / Embeddings
from llama_index.llms.openai import OpenAI as OpenAILLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Agente + memoria
from llama_index.core.agent.workflow import FunctionAgent
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.storage.chat_store import SimpleChatStore

# Reranker (SBERT cross-encoder)
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank


# =========================
# Configuración y constantes
# =========================
load_dotenv()

CSV_PATH = os.getenv("CSV_PATH", "data/rotten_tomatoes_movies.csv")
PERSIST_DIR = "storage_5"
os.makedirs(PERSIST_DIR, exist_ok=True)

# Archivos de persistencia
FINGERPRINT_FILE = os.path.join(PERSIST_DIR, ".embed_fingerprint.txt")
MANIFEST_PATH = os.path.join(PERSIST_DIR, "manifest.json")
CHAT_STORE_PATH = os.path.join(PERSIST_DIR, "chat_store.json")
CHAT_STORE_KEY = os.getenv("CHAT_STORE_KEY", "movies_rag_session")

# Modelos
OPENAI_LLM_MODEL = "gpt-4o-mini-2024-07-18"
EMBED_MODEL_ID = os.getenv("EMBED_MODEL_ID", "sentence-transformers/all-MiniLM-L6-v2")
RERANKER_MODEL_ID = os.getenv("RERANKER_MODEL_ID", "cross-encoder/ms-marco-MiniLM-L-6-v2")

# Parámetros RAG
RETRIEVER_TOP_K = int(os.getenv("RETRIEVER_TOP_K", "12"))
RERANKER_TOP_N = int(os.getenv("RERANKER_TOP_N", "4"))

# Memoria
MEMORY_TOKEN_LIMIT = int(os.getenv("MEMORY_TOKEN_LIMIT", "3900"))

# Logging base
logger = logging.getLogger("movies_rag_workflow")
logger.setLevel(logging.INFO)
_fmt = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
_ch = logging.StreamHandler()
_ch.setLevel(logging.INFO)
_ch.setFormatter(_fmt)
_fh = logging.FileHandler(os.path.join(PERSIST_DIR, "movies_rag_workflow.log"), encoding="utf-8")
_fh.setLevel(logging.INFO)
_fh.setFormatter(_fmt)
logger.addHandler(_ch)
logger.addHandler(_fh)


# =========================
# Utilidades de persistencia
# =========================
def file_sha256(path: str) -> str:
    """Hash del archivo para detectar cambios de datos."""
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()

def embed_fingerprint(provider: str, model_name: str) -> str:
    """Fingerprint breve del embedder (proveedor + modelo)."""
    return hashlib.sha256(f"{provider}:{model_name}".encode("utf-8")).hexdigest()[:16]

def read_text(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return ""

def write_text(path: str, data: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(data)

def load_manifest() -> dict:
    try:
        with open(MANIFEST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def save_manifest(d: dict) -> None:
    os.makedirs(PERSIST_DIR, exist_ok=True)
    with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
        json.dump(d, f, ensure_ascii=False, indent=2)


# =========================
# Datos esperados del CSV
# =========================
EXPECTED_COLS = [
    "rotten_tomatoes_link","movie_title","movie_info","critics_consensus",
    "content_rating","genres","directors","authors","actors",
    "original_release_date","streaming_release_date","runtime",
    "production_company","tomatometer_status","tomatometer_rating","tomatometer_count",
    "audience_status","audience_rating","audience_count",
    "tomatometer_top_critics_count","tomatometer_fresh_critics_count","tomatometer_rotten_critics_count",
]


# =========================
# Workflow
# =========================
class MoviesRAGWorkflow(Workflow):
    """
    Workflow por pasos:
      0) _stop_sentinel       -> retorna StopEvent (requisito del framework)
      1) configure_models     -> Settings.llm/embed/node_parser/callbacks  (StartEvent)
      2) load_dataframe       -> df (pandas)
      3) make_documents       -> docs (List[Document])
      4) build_or_load_index  -> index (VectorStoreIndex persistido en storage_5)
      5) build_tools_and_agent-> tools (rag_search con reranker; movie_stats), agent
      6) ensure_memory        -> memory (ChatMemoryBuffer persistente)
      7) answer               -> ejecuta agent.run(prompt, ctx, memory), persiste memoria
    """

    def __init__(self):
        super().__init__()
        logger.info("Inicializando MoviesRAGWorkflow")

    # ---------- Paso 0 (sentinela) ----------
    @step()
    async def _stop_sentinel(self, e: Event) -> StopEvent:
        """Cumple con el requisito: al menos un step debe retornar StopEvent."""
        return StopEvent()

    # ---------- Paso 1 ----------
    @step()
    async def configure_models(self, start: StartEvent, ctx: Context) -> None:
        """Configura LLM, embeddings, node parser y callbacks de debug (StartEvent requerido)."""
        logger.info("[WF] Paso 1: configurar modelos")
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("Falta OPENAI_API_KEY en el entorno o .env")

        Settings.llm = OpenAILLM(
            model=OPENAI_LLM_MODEL,
            api_key=api_key,
            temperature=0.2,
        )

        Settings.embed_model = HuggingFaceEmbedding(
            model_name=EMBED_MODEL_ID,
            device="cpu",  # "cuda" si tenés GPU
        )

        Settings.node_parser = SentenceSplitter(
            chunk_size=2048,
            chunk_overlap=64,
            include_metadata=False,
            include_prev_next_rel=False,
        )

        debug_handler = LlamaDebugHandler(print_trace_on_end=True)
        Settings.callback_manager = CallbackManager([debug_handler])

        logger.info("[WF] Modelos configurados: LLM=%s | EMBED=%s", OPENAI_LLM_MODEL, EMBED_MODEL_ID)

    # ---------- Paso 2 ----------
    @step()
    async def load_dataframe(self, e: Event, ctx: Context) -> pd.DataFrame:
        logger.info("[WF] Paso 2: cargar DataFrame desde %s", CSV_PATH)
        if not os.path.exists(CSV_PATH):
            raise FileNotFoundError(f"No se encontró el CSV en {CSV_PATH}")

        df = pd.read_csv(CSV_PATH)
        missing = [c for c in EXPECTED_COLS if c not in df.columns]
        if missing:
            raise ValueError(f"Faltan columnas requeridas: {missing}")

        for c in EXPECTED_COLS:
            if c in df.columns and c not in ("tomatometer_rating", "audience_rating"):
                df[c] = df[c].astype(str).fillna("")
        df["tomatometer_rating"] = pd.to_numeric(df["tomatometer_rating"], errors="coerce")
        df["audience_rating"] = pd.to_numeric(df["audience_rating"], errors="coerce")

        df["year"] = df["original_release_date"].apply(
            lambda s: (re.search(r"(\d{4})", s).group(1) if isinstance(s, str) and re.search(r"(\d{4})", s) else "")
        )

        df = df[df["movie_info"].astype(str).str.strip() != ""].reset_index(drop=True)
        logger.info("[WF] CSV cargado | Filas: %d | Columnas: %d", len(df), len(df.columns))
        return df

    # ---------- Paso 3 ----------
    @step()
    async def make_documents(self, e: Event, ctx: Context, df: pd.DataFrame) -> List[Document]:
        logger.info("[WF] Paso 3: construir Documents para RAG")
        docs: List[Document] = []
        for _, r in df.iterrows():
            parts = [
                f"Título: {r.get('movie_title','')}",
                f"Descripción: {r.get('movie_info','')}",
            ]
            if str(r.get("critics_consensus", "")).strip():
                parts.append(f"Consensus de críticos: {r.get('critics_consensus','')}")
            if str(r.get("genres","")).strip():
                parts.append(f"Géneros: {r.get('genres','')}")
            if str(r.get("directors","")).strip():
                parts.append(f"Directores: {r.get('directors','')}")
            if str(r.get("actors","")).strip():
                parts.append(f"Actores: {r.get('actors','')}")
            if pd.notna(r.get("tomatometer_rating")):
                parts.append(f"Tomatometer: {r.get('tomatometer_rating')}")
            if pd.notna(r.get("audience_rating")):
                parts.append(f"Audience score: {r.get('audience_rating')}")

            text = "\n".join(parts)
            md = {
                "movie_title": r.get("movie_title",""),
                "genres": r.get("genres",""),
                "directors": r.get("directors",""),
                "actors": r.get("actors",""),
                "year": r.get("year",""),
                "tomatometer_rating": r.get("tomatometer_rating", None),
                "audience_rating": r.get("audience_rating", None),
                "rotten_tomatoes_link": r.get("rotten_tomatoes_link",""),
            }
            docs.append(Document(text=text, metadata=md))

        logger.info("[WF] Documents creados: %d", len(docs))
        return docs

    # ---------- Paso 4 ----------
    @step()
    async def build_or_load_index(self, e: Event, ctx: Context, docs: List[Document]) -> VectorStoreIndex:
        logger.info("[WF] Paso 4: construir/cargar índice persistente en %s", PERSIST_DIR)
        provider = "hf"
        model_id = EMBED_MODEL_ID
        current_embed_fp = embed_fingerprint(provider, model_id)
        stored_embed_fp = read_text(FINGERPRINT_FILE)

        data_fp = file_sha256(CSV_PATH)
        chunk_cfg = {"chunk_size": 2048, "overlap": 64}
        desired_manifest = {
            "embed_fingerprint": current_embed_fp,
            "data_fingerprint": data_fp,
            "chunking": chunk_cfg,
            "docs_pipeline_version": "v1",
        }
        current_manifest = load_manifest()

        required = {"docstore.json", "index_store.json", "vector_store.json"}
        #storage_ok = os.path.isdir(PERSIST_DIR) and required.issubset(set(os.listdir(PERSIST_DIR)))

        #if storage_ok and current_manifest == desired_manifest and stored_embed_fp == current_embed_fp:
        if os.path.isdir(PERSIST_DIR):
            logger.info("[WF] Cargando índice existente (fingerprints OK)")
            storage_ctx = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
            index = load_index_from_storage(storage_ctx, embed_model=Settings.embed_model)
            return index

        logger.info("[WF] (Re)construyendo índice vectorial...")
        index = VectorStoreIndex.from_documents(docs, embed_model=Settings.embed_model)
        index.storage_context.persist(persist_dir=PERSIST_DIR)
        write_text(FINGERPRINT_FILE, current_embed_fp)
        save_manifest(desired_manifest)
        logger.info("[WF] Índice creado y persistido")
        return index

    # ---------- Paso 5 ----------
    @step()
    async def build_tools_and_agent(self, e: Event, ctx: Context, index: VectorStoreIndex, df: pd.DataFrame) -> FunctionAgent:
        logger.info("[WF] Paso 5: construir tools y agente")
        reranker = SentenceTransformerRerank(model=RERANKER_MODEL_ID, top_n=RERANKER_TOP_N)

        query_engine = index.as_query_engine(
            similarity_top_k=RETRIEVER_TOP_K,
            node_postprocessors=[reranker],
            verbose=True,
        )

        async def rag_search(query: str) -> str:
            t0 = datetime.now()
            resp = await query_engine.aquery(query)
            dt = (datetime.now() - t0).total_seconds()

            fuentes = []
            for sn in getattr(resp, "source_nodes", []) or []:
                meta = sn.node.metadata or {}
                title = meta.get("movie_title") or "(sin título)"
                score = getattr(sn, "score", None)
                try:
                    fuentes.append(f"- {title} (score={score:.3f})")
                except Exception:
                    fuentes.append(f"- {title}")

            answer = str(resp)
            if fuentes:
                answer += "\n\nFuentes recuperadas (RAG + rerank):\n" + "\n".join(fuentes)
            answer += f"\n\n[diag] top_k={RETRIEVER_TOP_K} | rerank_top_n={RERANKER_TOP_N} | latency={dt:.3f}s"
            return answer

        def movie_stats(
            question: str,
            metric: Literal["tomatometer_rating", "audience_rating"] = "tomatometer_rating",
            mode: Literal["top", "bottom"] = "bottom",
            n: int = 5,
            by: Optional[str] = None,
            value: Optional[str] = None,
        ) -> str:
            t0 = datetime.now()
            work = df.copy()

            if by and value:
                if by not in work.columns:
                    return f"⚠️ La columna '{by}' no existe."
                work = work[work[by].astype(str).str.contains(str(value), case=False, na=False)]

            if metric not in work.columns:
                return f"⚠️ La métrica '{metric}' no existe en el CSV."
            work[metric] = pd.to_numeric(work[metric], errors="coerce")
            work = work.dropna(subset=[metric])

            if work.empty:
                return "⚠️ No hay filas que coincidan con el filtro/métrica."

            ascending = (mode == "bottom")
            work = work.sort_values(metric, ascending=ascending).head(n)

            lines = [f"Resultados ({mode} por {metric}) - n={n}"]
            for _, r in work.iterrows():
                lines.append(
                    f"- {r.get('movie_title','')} | {metric}={r.get(metric,'')} | "
                    f"audience={r.get('audience_rating','')} | año={r.get('year','')} | "
                    f"director={r.get('directors','')}"
                )
            dt = (datetime.now() - t0).total_seconds()
            lines.append(f"[diag] Filas tras filtro: {len(work)} | latency={dt:.3f}s")
            return "\n".join(lines)

        agent = FunctionAgent(
            tools=[rag_search, movie_stats],
            llm=Settings.llm,
            system_prompt=(
                "Eres un asistente sobre películas basadas en un CSV de Rotten Tomatoes.\n"
                "- Si la pregunta es abierta (sinopsis/temas/elenco), usa 'rag_search'.\n"
                "- Si pide rankings o filtros de columnas (top/bottom, director, género, etc.), usa 'movie_stats'.\n"
                "Responde en español y, si usas RAG, lista 'Fuentes recuperadas'."
            ),
        )
        logger.info("[WF] Agent listo con 2 tools (RAG+rerank, stats tabular)")
        return agent

    # ---------- Paso 6 ----------
    @step()
    async def ensure_memory(self, e: Event, ctx: Context) -> ChatMemoryBuffer:
        logger.info("[WF] Paso 6: cargar/crear memoria persistente de chat")
        try:
            chat_store = SimpleChatStore.from_persist_path(CHAT_STORE_PATH)
        except Exception:
            chat_store = SimpleChatStore()

        memory = ChatMemoryBuffer.from_defaults(
            token_limit=MEMORY_TOKEN_LIMIT,
            chat_store=chat_store,
            chat_store_key=CHAT_STORE_KEY,
        )
        logger.info("[WF] Memoria lista (token_limit=%d)", MEMORY_TOKEN_LIMIT)
        return memory

    # ---------- Paso 7 ----------
    @step()
    async def answer(self, e: Event, ctx: Context, agent: FunctionAgent, memory: ChatMemoryBuffer, user_input: str) -> str:
        logger.info("[WF] Paso 7: responder consulta: %s", user_input)

        t0 = datetime.now()
        resp = await agent.run(user_input, ctx=ctx, memory=memory)
        try:
            memory.chat_store.persist(persist_path=CHAT_STORE_PATH)
        except Exception as ex:
            logger.warning("[WF] No se pudo persistir memoria: %s", ex)
        dt = (datetime.now() - t0).total_seconds()
        logger.info("[WF] Respuesta generada (latency=%.3fs)", dt)
        return str(resp)


# =========================
# CLI interactivo
# =========================
INTRO = """
Workflow RAG — CSV Rotten Tomatoes (Memoria persistente + Reranker)
-------------------------------------------------------------------
Ejemplos:
 - "¿De qué trata The Godfather?"
 - "Top 5 por audience_rating del director Christopher Nolan"
 - "¿Cuáles son las 10 peores por tomatometer_rating?"

Comandos: /salir | /hist | /reset
"""

async def run_cli():
    print(INTRO)

    wf = MoviesRAGWorkflow()
    start = StartEvent()
    e = Event()

    # Context provisional: ahora requiere el workflow
    provisional_ctx = Context(workflow=wf)

    # Bootstrap del workflow
    await wf.configure_models(start=start, ctx=provisional_ctx)
    df = await wf.load_dataframe(e=e, ctx=provisional_ctx)
    docs = await wf.make_documents(e=e, ctx=provisional_ctx, df=df)
    index = await wf.build_or_load_index(e=e, ctx=provisional_ctx, docs=docs)
    agent = await wf.build_tools_and_agent(e=e, ctx=provisional_ctx, index=index, df=df)

    # Context definitivo asociado al workflow
    ctx = Context(workflow=wf)

    memory = await wf.ensure_memory(e=e, ctx=ctx)

    # Loop de chat
    while True:
        q = input("Tú> ").strip()
        if not q:
            continue

        if q.lower() in {"/salir", "salir", "exit", "quit"}:
            try:
                memory.chat_store.persist(persist_path=CHAT_STORE_PATH)
            except Exception:
                pass
            print("¡Adiós!")
            break

        if q.lower() in {"/hist", "/history"}:
            msgs = memory.get()
            if not msgs:
                print("\nBot>\n(Historial vacío)\n")
            else:
                print("\nBot> Historial:")
                for m in msgs[-12:]:
                    role = "Tú" if getattr(m, "role", None) and getattr(m.role, "value", "") == "user" else "Bot"
                    content = getattr(m, "content", "")
                    if not isinstance(content, str):
                        content = str(content)
                    print(f"- {role}: {content[:200]}")
                print()
            continue

        if q.lower() in {"/reset"}:
            try:
                memory.chat_store.delete_messages(CHAT_STORE_KEY)
                memory.chat_store.persist(persist_path=CHAT_STORE_PATH)
                print("\nBot>\nMemoria borrada para esta sesión.\n")
            except Exception as ex:
                print(f"\nBot>\nNo se pudo borrar memoria: {ex}\n")
            continue

        try:
            ans = await wf.answer(e=e, ctx=ctx, agent=agent, memory=memory, user_input=q)
            print(f"\nBot>\n{ans}\n")
        except Exception as ex:
            logger.exception("[WF] Error en answer()")
            print(f"[ERROR] {ex}")


def main():
    try:
        asyncio.run(run_cli())
    except KeyboardInterrupt:
        print("\n(Interrumpido por usuario)")


if __name__ == "__main__":
    main()
